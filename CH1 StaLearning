I. What is statistical learning?
   Develope an accurate model that can be used to predict responses using predictors.
1. f(x)
   Y = f(x) + e
2. f^(x)
   We are trying to estimate f for prediction and inference purpose.
   Y^ = f^(x)
   f(x) = f^(x) + reducible error
   
   Y = f(x) + e = f^(x) + reducible error + e
3. We are focusing on bringing down the reducible error
   E(Y-Y^)^2 = reducible error + e
             = [f(x) -f^(x)]^2 + var(e)
             = E[f(x) + e - f^(x)]^2
             = E[Y-Y^]^2
4. How we estimate f? parametric and non-parametric
   parametric: make presumptions about the model, extimate the parameters. 
   non-parametric: need a very large number of observations. 
5. Trade-off between prediction accuracy and interpretability (flexibility has more variance, non_flexible has more bias)
6. Supervised and un-supervised learning
   unsupervised learning does not has the response value Y
7. Regression and Classification Problems
   regression - quantitatice response
   classification - qualitative response. We do use regression for binary quali by setting dummy varibales.
   
II. Testify Model Accuracy
 1. Measuring the quality of fitness, using: MSE (mean squared error)
    MSE: mean sqr error, MSE = (1/n) * sum from i=1 to i=n ( Yi-f^(Xi) )^2, sum sqr the diff between predicted and observed responses for Xi
    We wish a small MSE, we pick the model with the smallest TEST MSE
    We care about the TEST MSE, the training MSE usually always perform better than the test MSE.
    Notice the U-shape MSE and irreducible curve. The irreducible error it the minimum possible test MSE
    recall the E(Y-Y^)^2 = E[ f(x) + e - f^(x) ]^2 = [ f(x) - f^(x) ]^2  + var(e) = reducible error + irreducible error
    
 2. Trade-off between variance and bias
    bias comes from the simplification of complex problems, variance alters when changing different training data set
 
    more variance - comes from more flexible models. 
                    When fit the training too well, has the risk of overfitting. We care about the test MSE accordingly, the predictions.
    more bias - comes from less flexible models, less complexity, easy for interpretation. 
                May not fitting the data well, sometimes superisingly has better predictions.
                
    reacall the E(Y-Y^)^2 and apply (x0,y0), E[ y0 - f^(x0) ]^2 = [ f(x) - f^(x) ]^2             +  var(e)
                                                                =  reducible error               +  irreducible error
                                                                =  Var(f^(x0)) + Bias(f^(x0))^2  +  var(e)
 
 3. In Classification Cases
    quantifying the accuracy of the f^: 1/n * sum of I(yi not equals to y^i) from i = 1 to i = n
    


Process: 1.see data, 2.fit data in lm() and 3.summary() to see beta estimtes, 5.create the predict() model, 6. plot the data and model line, 7.plot the 4 tables
1. head()
2. lm(y~x,data)
   using attach(DataX), then all the following data would auto assign data = DataX,
   lm.fit = lm(y~x)
3. summary(lm.fit) gives almost all the useful sta info.
   notice: we wish a small p-value, p-value stands for the probability of geting a sample more extrem. We don't want extrem values, so a small p is good.
           we wish a small RSE, residual standard error is the error, it measures the fitness of our estimated model to the real one.
           we wish a large t-sta, t-statistic measures the number of std that our value (estimates) are away from 0.
           we wish a large R^2, it measures the proportion of variance that could be explained. R^2 scale: [0,1].
           we wish a large F-sta, it measures the aggregate fitness (cinsidering the corporation of multiple predictors), close to 1 means a bad model.
4. Print Specific Indicators
    names(lim.fit) to get some of the possible terms we could have
    confint(lm.fit) to get the confidence interval estimates instead of a single coef number
5. predict(lm.fit,data.fram())
    Confidence interval VS prediction interval. predi has e so larger range
       predict(lm.fit,data.frame (lstat=c(5,10,15)), interval='confidence')
       predict(lm.fit,data.frame (lstat=c(5,10,15)), interval='prediction')
6. plot the data and draw the prediction line
    plot(lstat,medv)
    abline(lm.fit,col='purple')
7. Draw the 4 plots in one page together
    par(mfrow=c(2,2))
    plot(lm.fit)
    7.1 Residuals VS Fitted Plot: a flat line(spread) show a good fit, while a parabola indicates there are non-linear relationships that are not explained by the model.
    7.2 Normal Q-Q Plot: indicates if the residuals are normally distributed. A straight dashed line would be prefered.
    7.3 Scale-Location Plot: Indicates if the residuals are spread equally along the ranges of predictors. Prefer horizontal line with dots qually spread. Use it to check the assumption of equal variance (homoscedasticity).
    7.4 Residuals VS Leverage Plot: This graph helps us find influential outliers. Not all the outliers are important. Looking for outliers that are outside the Cook's Distance on the right side of the graph. 
8. Multiple LR and interation Terms
    multi predictors: lm(y~x1 + x2); to include all predictors: lm(y~.), use VIF to evaluate the model: library(car) vif(lm.fit); delete 1 predicot: lm(y~. -x1)
    interactions: lm(y~x1*x2); x1*x2 --> x1 + x2 + x1:x2; x1:x2 means the interation term between x1 and x2
9. Non-Linear Predictors
    create I(x^2) as a new predictor, lm(y~ x + I(x^2))
    poly(x,5) gives x,x^2, x^3, x^4, x^5, lm(y~poly(x,5))
10. Qualitative perdicotrs
    contrasts(qualitative x) would auto provide us dummy variables
11. Compare Models
    Anova(lm.fit1, lm.fit2), prefer big F, small p,small error
    the anova performs a hypo test, hte null hypo is the two models fit the data same well.

    

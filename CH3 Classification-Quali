I. 
   In many situations, the response variables could be qualitative. One approach is classification. It predicte the probability of the oberservations that belongs to the qualitative categorization.
   5 types classifiers in the note: logistic regression, linear discrimizant analysis, quadratic discriminant analysis, naive Bayes, K-nearest enighbors
   Two reasons why not using linear Regression: For categorizations larger than 2, meaningless assigned numbers. For two cates, assign 1 and 0 but the estimated Y may exceed [0,1].

II. Logistic Regression, range [0,1]
1. The logi model for binary
   Logi func: p(X) = e^(b0 + b1*X) / 1 + e^(b0 + b1*X)
   write as:  p(X) = A/(1+A) if set A= e^(b0 + b1*X)
   trans to:  p(X) / ( 1-p(x) ) = A, here p(X) / ( 1-p(x) ) is called odds, [0,+infinity]
   both log:  log( p(X) / ( 1-p(x) ) ) = b0 + b1X, here log( p(X) / ( 1-p(x) ) ) is called log odds or say logit
              One unit Increasing in X changes the log odds by b1, b1 does not correspond to the change in p(X), change in p(X) depends on the value of X.
              But still, if b1 is positive, then the increaseing in X associates with increasing in p(X).
2. Estimate the regression coefficients
    To maximum likelihood: b0 and b1 such that predicted probability p^(Xi) using p(X) = e^(b0 + b1*X) / 1 + e^(b0 + b1*X) be as close as possible to each individuals' observed value
    In the R table, z-statistic just like t-sta in CH2, describes how many standard deviations the object is away from the population mean, wish small z
3. Making Predictions
    use the coefficient in R tables to calculate p^(x)
    
4. Extend to multipul logistic regression
   Logi func: p(X) = e^(b0 + b1*X1 +  b2*X2 + ... + bpXp) / 1 + e^(b0 + b1*X1 +  b2*X2 + ... + bpXp)
   write as:  p(X) = A/(1+A) if set A= e^(b0 + b1*X1 +  b2*X2 + ... + bpXp)
   trans to:  p(X) / ( 1-p(x) ) = A, here p(X) / ( 1-p(x) ) is called odds, [0,+infinity]
   both log:  log( p(X) / ( 1-p(x) ) ) = b0 + b1*X1 +  b2*X2 + ... + bpXp , here log( p(X) / ( 1-p(x) ) ) is called log odds or say logit

5. Multinomial logistic regression
   apply when classications of the response >= 3
   could use other alternative

III. Generative Models for Classification
1. process: seperatly model the distribution of predictors, then use Bayes' theorem to make an estimate fit in Pr(Y=k|X=x)
            When Xs' distributions are assumed normal, the model is similar to logistic regression
2. Advantages over logistic regression: logi reg is unstable when see substantial seperations between two classes; 
                                        generative models better when Xs' distributions normal and small sample size;
                                        generative models natually apply fo case of more than 2 response classes.
3.0 There are K classes for the response, K>=2,
    density function for X: fk(X) Ξ Pr(X|Y=k). X is a qualitative random variable. fk(x) large means highly possible observe x in kth class.
    Bayes' Theorem: Pr(Y=k |X=x) = ...
    pk(x) = Pr(Y=k |X=x), means the probability that x in kth class, given the predicted value of x
    Three ways to estimate the density function fk(x), has some simplification assumptions.
    
LDA has several means for each k predictor classes, but assuming one common covariance matrix; X ~ N(μk,Σ)
QDA has several means for each k predictor classes, and several covariance matrixs for each k predictor classes; X ~ N(μk,Σk)
Naive Bayes is similar to QDA but has the preassumption that the p predictors are independent within the kth class; X ~ N(μk,Σk); use when n is small
    
3.1 LDA,predictors=1; LDA, predictors>1
3.2 QDA, 
3.3 Naive Bayes




